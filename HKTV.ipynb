{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.wait import WebDriverWait"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the parameters of sub-category URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_category_list = [\"https://www.hktvmall.com/hktv/en/personalcarenhealth\", \"https://www.hktvmall.com/hktv/en/beautynhealth\"]\n",
    "url_code_list = []\n",
    "url_title_list = []\n",
    "url_category_list1 = []\n",
    "url_category_list2 = []\n",
    "\n",
    "for url_category in url_category_list:\n",
    "    res = requests.get(url_category)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    for url_code in soup.find_all(\"div\", class_=\"subnav\")[0].ul.find_all(\"a\", class_=\"link\"):\n",
    "        if url_code[\"data-cat\"].endswith(\"00000000\"):\n",
    "            try:\n",
    "                url_title_temp = url_code.text.split(\"&nbsp\")[1].replace(\",\",\"\").replace(\"&\",\"and\").replace(\" \",\"-\")\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            url_subtitle_temp = url_code.text.split(\"\\xa0\")[0].replace(\",\",\"\").replace(\"&\",\"and\").replace(\" \",\"-\")\n",
    "            url_title_list.append(url_title_temp+\"/\"+url_subtitle_temp)\n",
    "            url_code_list.append(url_code[\"data-cat\"])\n",
    "            if \"personal\" in url_category:\n",
    "                url_category_list1.append(\"Personal-Care-and-Health\")\n",
    "                url_category_list2.append(\"personalcarenhealth\")\n",
    "            elif \"beauty\" in url_category:\n",
    "                url_category_list1.append(\"Skincare-and-Makeup\")\n",
    "                url_category_list2.append(\"beautynhealth\")\n",
    "            else:\n",
    "                print(\"Check the updated structure !!!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the post URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_url_list = []\n",
    "post_category_list = []\n",
    "post_sub_category_list =[]\n",
    "post_page_list = []\n",
    "\n",
    "driver = webdriver.Chrome(service=Service('./chromedriver.exe'))\n",
    "driver.set_window_position(-1000, 0)\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "if len(url_code_list) == len(url_title_list) == len(url_category_list1) == len(url_category_list2):\n",
    "    for i in range(len(url_code_list)):\n",
    "        for page in range(3):\n",
    "            try:\n",
    "                url = f\"https://www.hktvmall.com/hktv/en/{url_category_list1[i]}/{url_category_list1[i]}/{url_title_list[i]}/main/search?page={page}&q=%3Arelevance%3Acategory%3A{url_code_list[i]}%3Azone%3A{url_category_list2[i]}%3Astreet%3Amain%3A\"\n",
    "                web_status = 0\n",
    "                while web_status == 0:\n",
    "                    driver.get(url)\n",
    "                    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                    if soup.img[\"src\"] != 'error.jpg':\n",
    "                        while \"All Rights Reserved\" not in soup.prettify():\n",
    "                            WebDriverWait(driver, 5)\n",
    "                            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                        post_url_list_temp = [j[\"data-id\"] for j in soup.find_all(\"div\", class_=\"product-brief\")]\n",
    "                        if len(post_url_list_temp)!=0:\n",
    "                            post_url_list.extend(post_url_list_temp)\n",
    "                            for count in range(len(post_url_list_temp)):\n",
    "                                post_category_list.append(url_category_list1[i].replace(\"-\", \" \"))\n",
    "                                post_sub_category_list.append(url_title_list[i].replace(\"-\", \" \").replace(\"/\", \" - \"))\n",
    "                                post_page_list.append(page)\n",
    "                            web_status +=1\n",
    "            except:\n",
    "                continue\n",
    "    driver.quit()\n",
    "else:\n",
    "    print(\"Check the parameter links of URL !!!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect each post data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_unit_list = [\"mg\", \"g\", \"kg\", \"ml\", \"l\", \"oz\", \"mm\", \"cm\", \"bag\", \"bottle\", \"box\", \"cap\", \"count\", \"day\", \"ea\", \"pack\", \"pc\", \"piece\", \"sachet\", \"set\", \"sheet\", \"stick\", \"tablet\", \"unit\", \"x\"]\n",
    "common_unit_list_zn = [\"毫克\", \"克\", \"千克\", \"毫升\", \"升\", \"安士\", \"毫米\", \"厘米\", \"袋\", \"瓶\", \"盒\", \"粒\", \"次\", \"天\", \"個\", \"包\", \"塊\", \"件\", \"袋\", \"組\", \"張\", \"支\", \"片\", \"單位\", \"項\"]\n",
    "\n",
    "driver = webdriver.Chrome(service=Service('./chromedriver.exe'))\n",
    "driver.set_window_position(-1000, 0)\n",
    "driver.maximize_window()\n",
    "\n",
    "for num in range(len(post_url_list)):\n",
    "    url = f\"https://www.hktvmall.com/hktv/en/p/{post_url_list[num]}\"\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    ## Part 1)\n",
    "    category = post_sub_category_list[num]\n",
    "    page_position = post_page_list[num]\n",
    "    brand_name = soup.find(\"h1\", class_=\"last\").text.split(\" - \")[0]\n",
    "    product_name = soup.find(\"h1\", class_=\"last\").text.split(\" - \")[1]\n",
    "\n",
    "    ## Part 2)\n",
    "    for i in soup.find(\"script\", type=\"application/ld+json\").text.split(\"\\n\"):\n",
    "        if \"image\" in i:\n",
    "            slider_img = re.findall(\"\\[\\S+\\]\", i)[0].replace(\"[\",\"\").replace(\"]\",\"\").replace('''\"''',\"\").split(\",\")\n",
    "            slider_img_list = [j for j in slider_img if j.endswith(\"96.jpg\") | j.endswith(\"96.png\")]\n",
    "\n",
    "    slider_video_list = []\n",
    "    for i in soup.find(\"div\", class_=\"product-img-slider\").script.text.split(\"\\n\"):\n",
    "        if (\"video\" in i) and len(re.findall(\"\\[\\S+\\]\", i))!=0:\n",
    "            slider_video = re.findall(\"\\[\\S+\\]\", i)[0].replace(\"[\",\"\").replace(\"]\",\"\").replace('''\"''',\"\")\n",
    "            slider_video_list.append(slider_video)\n",
    "    slider_video_list = list(set(slider_video_list))\n",
    "    \n",
    "    ## Part 3)\n",
    "    # Example reason for no scraping \"promotional\" class - https://www.hktvmall.com/hktv/en/p/H8912001_S_HH_8806334371807_01\n",
    "    product_price = float(soup.find(\"div\", class_=\"price\").text.replace(\"$\",\"\"))\n",
    "    if \"merchant\" not in soup.find(\"div\", class_=\"deliveryLabelText\").text:\n",
    "        delivery = soup.find(\"div\", class_=\"deliveryLabelText\").text.replace(\"-Day Delivery\", \" day(s)\")\n",
    "    else:\n",
    "        delivery_temp = soup.find(\"tr\", class_=\"product-return-delivery\").li.text.replace(\" \", \"\").replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"Days\", \" day(s)\")\n",
    "        delivery = delivery_temp[re.search(\"\\d\", delivery_temp).start():]\n",
    "    try:\n",
    "        sales_count_over = int(re.search(\"\\d+\\+\", soup.find(\"div\", class_=\"salesNumber-container\").span.text.replace(\",\", \"\")).group()[:-1])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    packing_spec_number_list = []\n",
    "    packing_spec_unit_list = []\n",
    "    packing_spec_temp_list = []\n",
    "    packing_spec_title = product_name.lower().replace(\" \",\"\")\n",
    "    for i in range(len(common_unit_list)):\n",
    "        packing_spec_title = packing_spec_title.replace(common_unit_list_zn[i], common_unit_list[i])\n",
    "    for i in soup.find_all(\"tr\", class_=\"productPackingSpec\"):\n",
    "        if \"Packing Spec\" in i.text:\n",
    "            for j in i.find_all(\"td\"):\n",
    "                if j.text != \"Packing Spec\":\n",
    "                    packing_spec = j.text.lower().replace(\",\",\"\").replace(\" \",\"\")\n",
    "                    for k in range(len(common_unit_list)):\n",
    "                        packing_spec = packing_spec.replace(common_unit_list_zn[k], common_unit_list[k])\n",
    "        if \"Country of Origin\" in i.text:\n",
    "            for j in i.find_all(\"span\"):\n",
    "                if j.text != \"Country of Origin\":\n",
    "                    country_of_origin = j.text\n",
    "\n",
    "    for unit in common_unit_list:\n",
    "        if re.search(f\"\\d+{unit}\", packing_spec_title) != None:\n",
    "            packing_spec_temp_list.append(re.search(f\"\\d+{unit}\", packing_spec_title).group())\n",
    "        if re.search(f\"\\d+{unit}\", packing_spec) != None:\n",
    "            packing_spec_temp_list.append(re.search(f\"\\d+{unit}\", packing_spec).group())\n",
    "    packing_spec_temp_list = list(set(packing_spec_temp_list))\n",
    "    for unit in common_unit_list:\n",
    "        for record in packing_spec_temp_list:\n",
    "            try:\n",
    "                packing_spec_number_list.append(re.search(f\"\\d+{unit}\", record).group().split(unit)[0])\n",
    "                packing_spec_unit_list.append(unit)\n",
    "            except:\n",
    "                continue\n",
    "    del(packing_spec)\n",
    "\n",
    "    remark = soup.find(\"tr\", class_=\"productRemark\").find(\"span\", class_=\"short-desc\").text\n",
    "\n",
    "    ## Part 4)\n",
    "    try:\n",
    "        description_video = len(soup.find(\"div\", id=\"videoTab\").find(\"ul\", class_=\"video-list\").find_all(\"li\"))\n",
    "    except:\n",
    "        description_video = 0\n",
    "    try:\n",
    "        description_img_list = [temp_url[\"src\"] for temp_url in soup.find(\"div\", id=\"photoTab\").find_all(\"img\")]\n",
    "    except:\n",
    "        description_img_list = []\n",
    "    description = soup.find(\"div\", id=\"descriptionsTab\").find(\"div\", class_=\"tabBody\").text.replace(\"\\n\", \"\")\n",
    "\n",
    "    ## Part 5)\n",
    "    url += \"?scrollTo=reviewTab\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        driver.find_element(By.CLASS_NAME, \"btnCloseLarge\").click()\n",
    "    except:\n",
    "        pass\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    \n",
    "    if soup.find(\"div\", class_=\"noReviews\")[\"style\"] != \"\":\n",
    "        customer_id = []\n",
    "        customer_name = []\n",
    "        customer_class = []\n",
    "        customer_rate = []\n",
    "        customer_review_date = []\n",
    "        customer_comment = []\n",
    "        customer_recommend = []\n",
    "        \n",
    "        next_page_checking = soup.find(\"a\", class_=\"next-btn\")[\"class\"]\n",
    "        while \"disable\" not in next_page_checking:\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            next_page_checking = soup.find(\"a\", class_=\"next-btn\")[\"class\"]\n",
    "            \n",
    "            for i in soup.find_all(\"div\", class_=\"product-review-wrapper\"):\n",
    "                customer_id.append(i.a.attrs[\"data-user\"])\n",
    "                customer_name.append(i.a.text)\n",
    "                if \"vip\" in i.find_all(\"span\")[1].attrs[\"class\"][0]:\n",
    "                    customer_class.append(i.find_all(\"span\")[1].attrs[\"class\"][0].replace(\"-highlight\",\"\"))\n",
    "                else:\n",
    "                    customer_class.append(\"normal\")\n",
    "                customer_rate.append(len(i.find_all(\"div\", class_=\"star-container\")[1]))\n",
    "                customer_review_date.append(i.find(\"span\", class_=\"review-date\").text)\n",
    "                customer_comment.append(i.find(\"div\", class_=\"review-title\").text)\n",
    "                try:\n",
    "                    if \"not\" not in i.find(\"span\", class_=\"recommendOrNot\").text:\n",
    "                        customer_recommend.append(1)\n",
    "                    else:\n",
    "                        customer_recommend.append(0)\n",
    "                except:\n",
    "                    customer_recommend.append(np.nan)\n",
    "        \n",
    "            next_page_button = driver.find_element(By.CLASS_NAME, \"next-btn\")\n",
    "            next_page_button.location_once_scrolled_into_view\n",
    "            action = ActionChains(driver)\n",
    "            action.key_down(Keys.ARROW_UP).key_up(Keys.ARROW_UP).perform()\n",
    "            next_page_button.click()\n",
    "            time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://brandirectory.com/search/{L'Oreal%20Paris} L'Oreal Paris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
